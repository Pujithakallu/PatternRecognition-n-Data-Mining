{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPiTfIbhCrJvVcBjvELValE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Pujitha kallu - w1653660\n","\n","\n","The goal is to explore how different proximity measures impact various types of analyses on the Iris dataset. Proximity measures are crucial for tasks like clustering, anomaly detection, and supervised classification. By using real data from the Iris dataset, we can uncover valuable insights.\n","\n","To start, we'll define the species mean vector for each Iris species. This involves computing the mean of the four-dimensional points associated with each species label. The dataset contains six columns, with one representing the label and one representing the ID, while the remaining four columns constitute the four dimensions of the data."],"metadata":{"id":"ymA7iX-W3Xcj"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2-CUWPWkWOn3","executionInfo":{"status":"ok","timestamp":1714684811613,"user_tz":420,"elapsed":28274,"user":{"displayName":"Pujitha Kallu","userId":"11726106518916817970"}},"outputId":"bcb1e5b3-7ad2-4d12-cb5d-fe8015668143"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","source":["1. Compute the Euclidean distance between all pairs of \"species mean vectors\" and report them in a table."],"metadata":{"id":"ld90jF3G-VTM"}},{"cell_type":"code","source":["import pandas as pd\n","from scipy.spatial.distance import pdist, squareform\n","\n","# Step 1: Read the data and filter out unnecessary columns\n","\n","file_path = '/content/drive/My Drive/Pujitha_kallu_PRDM/Assignment-1/iris.csv'\n","\n","# Read the CSV file\n","df = pd.read_csv(file_path)\n","\n","df = df.drop(columns=[\"id\"])  # Drop the 'id' column\n","\n","# Step 2: Compute the mean vector for each species\n","mean_vectors = df.groupby('label').mean()\n","\n","\n","print(\"Mean species vectors:\", mean_vectors )\n","\n","\n","# Step 3: Compute the Euclidean distance between each pair of mean vectors\n","distances = pdist(mean_vectors, metric='euclidean')\n","\n","# Step 4: Report the distances in a table\n","distance_matrix = squareform(distances)\n","distance_df = pd.DataFrame(distance_matrix, columns=mean_vectors.index, index=mean_vectors.index)\n","\n","# Print the distance table\n","print(\"Euclidean distance between species mean vectors:\")\n","print(distance_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwd9XqM7a2zk","executionInfo":{"status":"ok","timestamp":1714606521014,"user_tz":420,"elapsed":134,"user":{"displayName":"Pujitha Kallu","userId":"11726106518916817970"}},"outputId":"7836482a-522a-4786-d83f-7deec0d8880a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean species vectors:                  Sepal length  Sepal width  Petal length  Petal width\n","label                                                                \n","Iris-setosa             5.006        3.418         1.464        0.244\n","Iris-versicolor         5.936        2.770         4.260        1.326\n","Iris-virginica          6.588        2.974         5.552        2.026\n","Euclidean distance between species mean vectors:\n","label            Iris-setosa  Iris-versicolor  Iris-virginica\n","label                                                        \n","Iris-setosa         0.000000         3.205175        4.752592\n","Iris-versicolor     3.205175         0.000000        1.620489\n","Iris-virginica      4.752592         1.620489        0.000000\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"Yw3pWjpM-06I"}},{"cell_type":"markdown","source":["2. Same as 1 except that standardize each of the column vectors (of the 4 dimensions first). Use the z-score standardization, i.e. subtract the column vector's mean from the value and divide by the column vector's standard deviation).\n"],"metadata":{"id":"gU2LEm0N-hoi"}},{"cell_type":"code","source":["import pandas as pd\n","from scipy.spatial.distance import pdist, squareform\n","from sklearn.preprocessing import StandardScaler\n","\n","# Step 1: Read the data and filter out unnecessary columns\n","\n","file_path = '/content/drive/My Drive/Pujitha_kallu_PRDM/Assignment-1/iris.csv'\n","\n","# Read the CSV file\n","df = pd.read_csv(file_path)\n","df = df.drop(columns=[\"id\"])  # Drop the 'id' column\n","\n","# Standardize the data using z-score standardization\n","scaler = StandardScaler()\n","df_std = pd.DataFrame(scaler.fit_transform(df.iloc[:, :-1]), columns=df.columns[:-1])\n","df_std['label'] = df['label']\n","\n","# Compute the mean vector for each species\n","mean_vectors = df_std.groupby('label').mean()\n","\n","# Compute the Euclidean distance between each pair of mean vectors\n","distances = pdist(mean_vectors, metric='euclidean')\n","\n","# Report the distances in a table\n","distance_df = pd.DataFrame(squareform(distances), columns=mean_vectors.index, index=mean_vectors.index)\n","print(\"Euclidean distance between species mean vectors (after z-score standardization):\")\n","print(distance_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VRVlsTqIhAKv","executionInfo":{"status":"ok","timestamp":1714437252968,"user_tz":420,"elapsed":123,"user":{"displayName":"Pujitha Kallu","userId":"11726106518916817970"}},"outputId":"8797b8fc-9f23-4d14-faa1-fe3e3ace0362"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Euclidean distance between species mean vectors (after z-score standardization):\n","label            Iris-setosa  Iris-versicolor  Iris-virginica\n","label                                                        \n","Iris-setosa         0.000000         2.840756        3.952601\n","Iris-versicolor     2.840756         0.000000        1.494566\n","Iris-virginica      3.952601         1.494566        0.000000\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"9Q_MV5kh-2vY"}},{"cell_type":"markdown","source":["3. Same as 1 except use Mahalanobis Distance."],"metadata":{"id":"BrV6dBG5_iTZ"}},{"cell_type":"code","source":["#for each pair of species\n","import pandas as pd\n","import numpy as np\n","\n","# Step 1: Read the data and filter out unnecessary columns\n","file_path = '/content/drive/My Drive/Pujitha_kallu_PRDM/Assignment-1/iris.csv'\n","df = pd.read_csv(file_path)\n","df = df.drop(columns=[\"id\"])  # Drop the 'id' column\n","\n","# Step 2: Compute the covariance matrix for each species\n","cov_matrices = {}\n","\n","for species in df['label'].unique():\n","    subset = df[df['label'] == species].iloc[:, :-1]  # Exclude the label column\n","    cov_matrix = np.cov(subset, rowvar=False)\n","    cov_matrices[species] = cov_matrix\n","\n","# Step 3: Calculate the Mahalanobis distance for each pair of species\n","mahalanobis_distances = pd.DataFrame(index=cov_matrices.keys(), columns=cov_matrices.keys())\n","\n","for species_i, cov_matrix_i in cov_matrices.items():\n","    for species_j, cov_matrix_j in cov_matrices.items():\n","        if species_i != species_j:\n","            # Compute the mean vectors\n","            mean_vector_i = df[df['label'] == species_i].iloc[:, :-1].mean().values\n","            mean_vector_j = df[df['label'] == species_j].iloc[:, :-1].mean().values\n","\n","            # Compute the Mahalanobis distance\n","            mean_diff = mean_vector_i - mean_vector_j\n","            mahalanobis_dist = np.sqrt(np.dot(np.dot(mean_diff, np.linalg.inv(cov_matrix_i)), mean_diff.T))\n","            mahalanobis_distances.loc[species_i, species_j] = mahalanobis_dist\n","\n","# Print the Mahalanobis distances\n","print(\"Mahalanobis Distances between species:\")\n","print(mahalanobis_distances)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kYTPih3xR5oc","executionInfo":{"status":"ok","timestamp":1714686621061,"user_tz":420,"elapsed":15,"user":{"displayName":"Pujitha Kallu","userId":"11726106518916817970"}},"outputId":"fab98db0-e289-403d-de6e-a79535285c3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mahalanobis Distances between species:\n","                Iris-setosa Iris-versicolor Iris-virginica\n","Iris-setosa             NaN       18.194879      26.905889\n","Iris-versicolor   10.131561             NaN       4.226902\n","Iris-virginica    12.972528        3.720048            NaN\n"]}]},{"cell_type":"code","source":["#for entire dataset\n","import pandas as pd\n","from scipy.spatial.distance import mahalanobis\n","import numpy as np\n","\n","# Step 1: Read the data and filter out unnecessary columns\n","file_path = '/content/drive/My Drive/Pujitha_kallu_PRDM/Assignment-1/iris.csv'\n","df = pd.read_csv(file_path)\n","df = df.drop(columns=[\"id\"])  # Drop the 'id' column\n","\n","# Step 2: Compute the mean vector for each species\n","mean_vectors = df.groupby('label').mean()\n","\n","# Step 3: Compute the covariance matrix for the entire dataset\n","cov_matrix = np.cov(df.iloc[:, :-1], rowvar=False)\n","\n","# Step 4: Calculate the Mahalanobis distance between each pair of mean vectors\n","mahalanobis_distances = pd.DataFrame(index=mean_vectors.index, columns=mean_vectors.index)\n","\n","for species1 in mean_vectors.index:\n","    for species2 in mean_vectors.index:\n","        mean_diff = mean_vectors.loc[species1] - mean_vectors.loc[species2]\n","        mahalanobis_dist = np.sqrt(np.dot(np.dot(mean_diff, np.linalg.inv(cov_matrix)), mean_diff.T))\n","        mahalanobis_distances.loc[species1, species2] = mahalanobis_dist\n","\n","print(\"Mahalanobis Distance between species mean vectors:\")\n","print(mahalanobis_distances)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VrgTGTcuBMKW","executionInfo":{"status":"ok","timestamp":1714684838029,"user_tz":420,"elapsed":1850,"user":{"displayName":"Pujitha Kallu","userId":"11726106518916817970"}},"outputId":"242eb138-a47f-498b-96fe-2634b941d3f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.68569351 -0.03926846  1.27368233  0.5169038 ]\n"," [-0.03926846  0.18800403 -0.32171275 -0.11798121]\n"," [ 1.27368233 -0.32171275  3.11317942  1.29638747]\n"," [ 0.5169038  -0.11798121  1.29638747  0.58241432]]\n","Mahalanobis Distance between species mean vectors:\n","label           Iris-setosa Iris-versicolor Iris-virginica\n","label                                                     \n","Iris-setosa             0.0        1.844099        2.35485\n","Iris-versicolor    1.844099             0.0        1.29136\n","Iris-virginica      2.35485         1.29136            0.0\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"4BpBTFLU-6SQ"}},{"cell_type":"markdown","source":[" 4. Same as 1 except use Cosine Similarity."],"metadata":{"id":"-b9QNsPgEnnj"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Step 1: Read the data and filter out unnecessary columns\n","file_path = '/content/drive/My Drive/Pujitha_kallu_PRDM/Assignment-1/iris.csv'\n","\n","# Read the CSV file\n","df = pd.read_csv(file_path)\n","df = df.drop(columns=[\"id\"])  # Drop the 'id' column\n","\n","# Step 2: Compute the mean vector for each species\n","mean_vectors = df.groupby('label').mean()\n","\n","# Step 3: Compute the cosine similarity between each pair of mean vectors\n","similarities = cosine_similarity(mean_vectors)\n","\n","# Step 4: Report the similarities in a table\n","similarity_df = pd.DataFrame(similarities, index=mean_vectors.index, columns=mean_vectors.index)\n","\n","# Print the similarity table\n","print(\"Table of Cosine Similarities:\")\n","print(similarity_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUHLYqPFEnXp","executionInfo":{"status":"ok","timestamp":1714546137426,"user_tz":420,"elapsed":544,"user":{"displayName":"Pujitha Kallu","userId":"11726106518916817970"}},"outputId":"f08c4c63-52d6-4f44-b9ef-1cf1bc4ccf04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Table of Cosine Similarities:\n","label            Iris-setosa  Iris-versicolor  Iris-virginica\n","label                                                        \n","Iris-setosa         1.000000         0.924848        0.888438\n","Iris-versicolor     0.924848         1.000000        0.995713\n","Iris-virginica      0.888438         0.995713        1.000000\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"dBTJxnIu-71e"}},{"cell_type":"markdown","source":["5.Same as 1 except use Pearson Correlation.\n","\n"],"metadata":{"id":"LwsJsPGoG8N2"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Define the file path\n","file_path = '/content/drive/My Drive/Pujitha_kallu_PRDM/Assignment-1/iris.csv'\n","\n","# Function to read the data and drop unnecessary columns\n","def read_and_drop_columns(file_path):\n","    df = pd.read_csv(file_path)\n","    return df.drop(columns=[\"id\"])\n","\n","# Function to standardize the data\n","def standardize_data(df):\n","    standardized_df = df.copy()\n","    for column in df.columns[:-1]:  # Exclude the label column\n","        mean = df[column].mean()\n","        std = df[column].std()\n","        standardized_df[column] = (df[column] - mean) / std\n","    return standardized_df\n","\n","# Function to compute covariance matrices for each species\n","def compute_corr_matrices(df):\n","    corr_matrices = {}\n","    for species in df['label'].unique():\n","        subset = df[df['label'] == species].iloc[:, :-1]  # Exclude the label column\n","        corr_matrices[species] = subset.corr()\n","    return corr_matrices\n","\n","# Read the data and drop unnecessary columns\n","df = read_and_drop_columns(file_path)\n","\n","# Step 2: Standardize the data\n","standardized_df = standardize_data(df)\n","\n","# Step 3: Compute the correlation matrix for each species\n","corr_matrices = compute_corr_matrices(standardized_df)\n","\n","# Print the correlation matrices for each species\n","for species, corr_matrix in corr_matrices.items():\n","    print(f\"Correlation matrix for {species}:\")\n","    print(corr_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4fu3dhlUOOi","executionInfo":{"status":"ok","timestamp":1714687314402,"user_tz":420,"elapsed":615,"user":{"displayName":"Pujitha Kallu","userId":"11726106518916817970"}},"outputId":"ef978cab-e205-4678-8ae7-964f2e6c4b65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Correlation matrix for Iris-setosa:\n","              Sepal length  Sepal width  Petal length  Petal width\n","Sepal length      1.000000     0.746780      0.263874     0.279092\n","Sepal width       0.746780     1.000000      0.176695     0.279973\n","Petal length      0.263874     0.176695      1.000000     0.306308\n","Petal width       0.279092     0.279973      0.306308     1.000000\n","Correlation matrix for Iris-versicolor:\n","              Sepal length  Sepal width  Petal length  Petal width\n","Sepal length      1.000000     0.525911      0.754049     0.546461\n","Sepal width       0.525911     1.000000      0.560522     0.663999\n","Petal length      0.754049     0.560522      1.000000     0.786668\n","Petal width       0.546461     0.663999      0.786668     1.000000\n","Correlation matrix for Iris-virginica:\n","              Sepal length  Sepal width  Petal length  Petal width\n","Sepal length      1.000000     0.457228      0.864225     0.281108\n","Sepal width       0.457228     1.000000      0.401045     0.537728\n","Petal length      0.864225     0.401045      1.000000     0.322108\n","Petal width       0.281108     0.537728      0.322108     1.000000\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from scipy.stats import pearsonr\n","\n","# Step 1: Read the data and filter out unnecessary columns\n","file_path = '/content/drive/My Drive/Pujitha_kallu_PRDM/Assignment-1/iris.csv'\n","df = pd.read_csv(file_path)\n","df = df.drop(columns=[\"id\"])  # Drop the 'id' column\n","\n","# Step 2: Compute the mean vector for each species\n","mean_vectors = df.groupby('label').mean()\n","\n","# Step 3: Compute the Pearson correlation between each pair of mean vectors\n","correlation_df = pd.DataFrame(index=mean_vectors.index, columns=mean_vectors.index)\n","for species1 in mean_vectors.index:\n","    for species2 in mean_vectors.index:\n","        if species1 != species2:\n","            correlation, _ = pearsonr(mean_vectors.loc[species1], mean_vectors.loc[species2])\n","            correlation_df.loc[species1, species2] = correlation\n","\n","# Replace NaN values with 0\n","correlation_df = correlation_df.fillna(0)\n","\n","# Print the correlation table\n","print(\"Pearson correlation between species mean vectors:\")\n","print(correlation_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7n-w66c5M6Ex","executionInfo":{"status":"ok","timestamp":1714546018510,"user_tz":420,"elapsed":735,"user":{"displayName":"Pujitha Kallu","userId":"11726106518916817970"}},"outputId":"59acfe62-6568-43e7-ea17-dfd43edc9038"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pearson correlation between species mean vectors:\n","label            Iris-setosa  Iris-versicolor  Iris-virginica\n","label                                                        \n","Iris-setosa         0.000000         0.763854        0.618438\n","Iris-versicolor     0.763854         0.000000        0.979489\n","Iris-virginica      0.618438         0.979489        0.000000\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"A3WQnogn-9Lj"}},{"cell_type":"markdown","source":["6. You now have five tables. Explain the pros and cons of each of the measures for the purpose of quantifying how similar pairs of species are."],"metadata":{"id":"9Blb5HxkG684"}},{"cell_type":"markdown","source":["Ans:\n","\n","1. Euclidean Distance:\n","   - Pros:\n","     - Straightforward calculation: Easy to compute and understand, providing a simple measure of distance between points in a multidimensional space.\n","     - Intuitive interpretation: Smaller distances imply greater similarity, while larger distances indicate greater dissimilarity.\n","   - **Cons**:\n","     - Sensitive to scale: Treats all dimensions equally, making it sensitive to differences in scale between features. Standardizing the data can mitigate this issue.\n","     - Not robust to outliers: Outliers can significantly influence the Euclidean distance, especially in high-dimensional spaces.\n","\n","2. Standardized Euclidean Distance:\n","   - Pros:\n","     - Scale-invariant: Standardizing the data ensures that all features have a mean of 0 and standard deviation of 1, making the distance measure less sensitive to differences in scale.\n","     - Compensates for variable scales: Provides a more accurate measure of similarity by removing the influence of variable scales.\n","   - Cons:\n","     - May lose interpretability: After standardization, the resulting distances may be more challenging to interpret in the original feature space.\n","\n","3. Mahalanobis Distance:\n","   - Pros:\n","     - Accounts for feature correlations: Takes into account the correlations between features, providing a more accurate measure of similarity for correlated data.\n","     - Scale-invariant and robust to outliers: Adjusts for differences in scale and is less sensitive to outliers compared to the Euclidean distance.\n","   - Cons:\n","     - Requires estimation of covariance matrix: Relies on estimating the covariance matrix from the data, which can be challenging, especially with limited samples.\n","     - Assumes multivariate normality: Assumes that the data follows a multivariate normal distribution, which may not hold true for all datasets.\n","\n","4. Cosine Similarity:\n","   - Pros:\n","     - Ignores magnitude: Focuses on the direction of vectors rather than their magnitude, making it suitable for comparing documents or text data.\n","     - Effective for high-dimensional sparse data: Well-suited for high-dimensional datasets where many features are zero or missing.\n","   - Cons:\n","     - Limited to non-negative data: Requires non-negative data as negative values can affect similarity calculations.\n","     - Does not consider feature correlations: Ignores correlations between features, which may be important in some contexts.\n","\n","5. Pearson Correlation:\n","   - Pros:\n","     - Measures linear relationship: Quantifies the strength and direction of the linear relationship between two variables, providing insight into linear associations between features.\n","     - Widely applicable: Commonly used in various statistical analyses and machine learning tasks.\n","   - Cons:\n","     - Assumes linear relationship: Assumes that the relationship between variables is linear, which may not hold true for all datasets.\n","     - Sensitive to outliers: Outliers can significantly affect correlation coefficients, potentially skewing similarity measures.\n","\n"],"metadata":{"id":"mE5HR1YSNPuV"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"hKoqktrr-_Cr"}},{"cell_type":"markdown","source":["7. For each of the species, find if there are outliers in the species. For this purpose compare the 4-dimensional vector of any particular object (row in the csv file) with the 4-dimensional mean vector of the species. Use Mahalanobis Distance as the proximity measure. You will need to choose appropriate thresholds for discerning whether a data point is anomalous or not. Explain how you did this."],"metadata":{"id":"2CmjzB9zHFj-"}},{"cell_type":"markdown","source":["**Ans**:To determine the threshold for identifying outliers using the Mahalanobis distance, we utilize the Chi-Square distribution, which provides critical values for a given significance level (alpha) and degrees of freedom. Here's the theoretical explanation of how we calculate this threshold:\n","\n","1. Significance Level (alpha): The significance level (alpha) is a predefined threshold that determines the probability of rejecting the null hypothesis when it is actually true. In the context of outlier detection, it represents the maximum tolerable probability of falsely identifying a non-outlying point as an outlier. Common choices for alpha include 0.05 (5% significance level) or 0.01 (1% significance level), but it can be adjusted based on the specific requirements and the desired stringency of the outlier detection process.\n","\n","2. Degrees of Freedom: The degrees of freedom correspond to the number of dimensions in the dataset. In our case, since we are dealing with a 4-dimensional dataset (4 features), the degrees of freedom are set to 4.\n","\n","3. Chi-Square Distribution: The Chi-Square distribution is a probability distribution that arises in statistical inference, particularly in hypothesis testing and confidence interval estimation for variance. The critical values of the Chi-Square distribution depend on the degrees of freedom and the significance level (alpha).\n","\n","4. Chi-Square Critical Value: Using the `chi2.ppf()` function from the SciPy library, we calculate the Chi-Square critical value corresponding to the desired significance level (alpha) and degrees of freedom. This critical value represents the threshold beyond which data points are considered outliers. For instance, if we choose a 5% significance level (alpha = 0.05), we obtain the Chi-Square critical value such that 95% of the distribution falls below this value.\n","\n","5. Threshold for Mahalanobis Distance:The Mahalanobis distance is calculated for each data point in the dataset. If the Mahalanobis distance of a data point exceeds the Chi-Square critical value, it is considered an outlier. The threshold derived from the Chi-Square distribution ensures that the outlier detection process adheres to the chosen significance level, thereby controlling the rate of false positives.\n","\n"],"metadata":{"id":"-t4pENC_TsrP"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from scipy.stats import chi2\n","from numpy.linalg import inv\n","from scipy.spatial.distance import mahalanobis\n","\n","# Read the data\n","iris_data = pd.read_csv('/content/drive/My Drive/Pujitha_kallu_PRDM/Assignment-1/iris.csv')\n","\n","# Mean vectors for each species\n","mean_vectors = {}\n","for species in iris_data['label'].unique():\n","    mean_vectors[species] = iris_data[iris_data['label'] == species][['Sepal length', 'Sepal width', 'Petal length', 'Petal width']].mean().values\n","\n","# Degrees of freedom (number of dimensions)\n","degrees_of_freedom = 4\n","\n","# Significance level (1 - alpha) for outlier detection\n","alpha = 0.45\n","\n","# Calculate the Chi-Square critical value for the given alpha and degrees of freedom\n","chi2_critical_value = chi2.ppf(1 - alpha, degrees_of_freedom)\n","\n","# Dictionary to store outliers for each species\n","outliers = {}\n","\n","# Detect outliers for each species using Mahalanobis distance\n","for species, mean_vector in mean_vectors.items():\n","    # Check if there are enough data points for this species\n","    if len(iris_data[iris_data['label'] == species]) < degrees_of_freedom + 1:\n","        print(f\"Not enough data points for species {species}. Skipping outlier detection.\")\n","        continue\n","\n","    # Calculate the inverse of the covariance matrix for the current species\n","    species_cov_matrix = iris_data[iris_data['label'] == species][['Sepal length', 'Sepal width', 'Petal length', 'Petal width']].cov()\n","    inv_species_cov_matrix = inv(species_cov_matrix)\n","\n","    # Calculate Mahalanobis distance for each data point of the current species\n","    distances = []\n","    for _, row in iris_data[iris_data['label'] == species][['Sepal length', 'Sepal width', 'Petal length', 'Petal width']].iterrows():\n","        data_point = row.values\n","        mahalanobis_distance = mahalanobis(data_point, mean_vector, inv_species_cov_matrix)\n","        distances.append(mahalanobis_distance)\n","\n","    # Determine outliers based on the threshold\n","    outliers[species] = iris_data[iris_data['label'] == species][distances > chi2_critical_value]\n","\n","# Print outliers for each species\n","for species, outlier_data in outliers.items():\n","    print(f\"Outliers for {species}:\")\n","    print(outlier_data.to_string(index=True))\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GF5YqEmnHJtG","executionInfo":{"status":"ok","timestamp":1714461393738,"user_tz":420,"elapsed":12,"user":{"displayName":"Pujitha Kallu","userId":"11726106518916817970"}},"outputId":"59ca74d1-1a8f-42b3-faa5-b74d69247112"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Outliers for Iris-setosa:\n","Empty DataFrame\n","Columns: [Sepal length, Sepal width, Petal length, Petal width, id, label]\n","Index: []\n","Outliers for Iris-versicolor:\n","Empty DataFrame\n","Columns: [Sepal length, Sepal width, Petal length, Petal width, id, label]\n","Index: []\n","Outliers for Iris-virginica:\n","     Sepal length  Sepal width  Petal length  Petal width      id           label\n","118           7.7          2.6           6.9          2.3  id_119  Iris-virginica\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"t6AiLdIY_Cxb"}},{"cell_type":"markdown","source":["8. Same as 7 except use Euclidean distance as the proximity measure. Do not\n","standardize the column vectors. You will need to choose appropriate thresholds for discerning whether a data point is anomalous or not. Explain how you did this."],"metadata":{"id":"N9L2eEPeHKJV"}},{"cell_type":"markdown","source":["**Ans:** When using Euclidean distance as the proximity measure for outlier detection without standardizing the column vectors, we typically rely on statistical measures such as the Interquartile Range (IQR) to determine appropriate thresholds for identifying anomalous data points. Here's how this process works:\n","\n","1. **Interquartile Range (IQR):** The IQR is a measure of statistical dispersion, representing the range between the first quartile (Q1, 25th percentile) and the third quartile (Q3, 75th percentile) of a dataset. It provides insights into the spread of the data, particularly in the middle 50% of the distribution, making it robust against outliers.\n","\n","2. **Euclidean Distance Calculation:** For each data point in the dataset, we compute its Euclidean distance from the centroid or mean vector of its respective group (e.g., species in the Iris dataset). The Euclidean distance is a direct measure of the straight-line distance between two points in a multi-dimensional space.\n","\n","3. **Threshold Determination:** We use the IQR of the Euclidean distances for each group (e.g., species) to establish thresholds for identifying outliers. The IQR serves as a robust measure of variability, capturing the spread of the distances within the dataset.\n","\n","4. **Outlier Detection:** Any data point with a Euclidean distance greater than Q3 + k × IQR or less than Q1 - k × IQR can be considered an outlier, where k is a multiplier representing the desired level of stringency in outlier detection. Common choices for k include 1.5 or 2, with larger values indicating a higher tolerance for extreme outliers.\n","\n","5. **Adjusting Thresholds:** The choice of k allows us to adjust the sensitivity of the outlier detection algorithm. Higher values of k result in broader thresholds, capturing more data points as outliers, while lower values of k lead to narrower thresholds, identifying only the most extreme outliers.\n","\n"],"metadata":{"id":"TDhMJUjs_RNc"}},{"cell_type":"code","source":["import pandas as pd\n","from scipy.spatial.distance import euclidean\n","\n","# Read the data\n","iris_data = pd.read_csv('/content/drive/My Drive/Pujitha_kallu_PRDM/Assignment-1/iris.csv')\n","\n","# Mean vectors for each species\n","mean_vectors = {}\n","for species in iris_data['label'].unique():\n","    mean_vectors[species] = iris_data[iris_data['label'] == species][['Sepal length', 'Sepal width', 'Petal length', 'Petal width']].mean().values\n","\n","# Dictionary to store outliers for each species\n","outliers = {}\n","\n","# Constant for determining the threshold (k times IQR)\n","k = 1.5\n","\n","# Dictionary to store outlier counts for each species\n","outlier_counts = {}\n","\n","# Detect outliers for each species using Euclidean distance without standardization\n","for species, mean_vector in mean_vectors.items():\n","    # Calculate Euclidean distance for each data point of the current species\n","    distances = []\n","    for _, row in iris_data[iris_data['label'] == species][['Sepal length', 'Sepal width', 'Petal length', 'Petal width']].iterrows():\n","        data_point = row.values\n","        euclidean_distance = euclidean(data_point, mean_vector)\n","        distances.append(euclidean_distance)\n","    # Calculate quartiles and IQR\n","    q1 = pd.Series(distances).quantile(0.25)\n","    q3 = pd.Series(distances).quantile(0.75)\n","    iqr = q3 - q1\n","    # Reset index of the DataFrame\n","    species_data = iris_data[iris_data['label'] == species].reset_index(drop=True)\n","    # Determine outliers based on the threshold (Q3 + k * IQR)\n","    outliers[species] = species_data[pd.Series(distances) > (q3 + k * iqr)]\n","    # Count outliers\n","    outlier_counts[species] = len(outliers[species])\n","\n","# Print outliers and their counts for each species\n","for species, outlier_data in outliers.items():\n","    print(f\"Outliers for {species}:\")\n","    print(outlier_data.to_string(index=True))\n","    print(f\"Number of outliers: {outlier_counts[species]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m4uesU3DzQgv","executionInfo":{"status":"ok","timestamp":1714695248062,"user_tz":420,"elapsed":801,"user":{"displayName":"Pujitha Kallu","userId":"11726106518916817970"}},"outputId":"02ac1eab-58fa-4088-8144-3eb4581de1b5"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Outliers for Iris-setosa:\n","    Sepal length  Sepal width  Petal length  Petal width     id        label\n","15           5.7          4.4           1.5          0.4  id_16  Iris-setosa\n","41           4.5          2.3           1.3          0.3  id_42  Iris-setosa\n","Number of outliers: 2\n","Outliers for Iris-versicolor:\n","    Sepal length  Sepal width  Petal length  Petal width     id            label\n","7            4.9          2.4           3.3          1.0  id_58  Iris-versicolor\n","10           5.0          2.0           3.5          1.0  id_61  Iris-versicolor\n","43           5.0          2.3           3.3          1.0  id_94  Iris-versicolor\n","48           5.1          2.5           3.0          1.1  id_99  Iris-versicolor\n","Number of outliers: 4\n","Outliers for Iris-virginica:\n","    Sepal length  Sepal width  Petal length  Petal width      id           label\n","6            4.9          2.5           4.5          1.7  id_107  Iris-virginica\n","18           7.7          2.6           6.9          2.3  id_119  Iris-virginica\n","Number of outliers: 2\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"o3tBYPGCB8pq"}},{"cell_type":"markdown","source":["9. Analyze your anomaly detection results. Does Mahalanobis distance work better than Euclidean distance? Or the other way around? Or the results are inconclusive?"],"metadata":{"id":"YZn8LLZCHN9A"}},{"cell_type":"markdown","source":["**Ans:** Certainly! Let's elaborate further on the comparison between Mahalanobis distance and Euclidean distance for outlier detection in the Iris dataset.\n","\n","**Mahalanobis Distance:**\n","\n","Advantages:\n","- Mahalanobis distance accounts for variable correlations, which is beneficial for datasets where variables are correlated. It provides a more accurate measure of dissimilarity in such cases.\n","- By considering the covariance between variables, Mahalanobis distance can offer insights into the true distance between data points.\n","\n","Limitations:\n","- It assumes that the data follows a multivariate normal distribution. If the data significantly deviates from this assumption, Mahalanobis distance might not perform well.\n","- Mahalanobis distance is sensitive to outliers in the dataset, which can skew the estimation of the covariance matrix and affect the accuracy of the distance calculations.\n","\n","**Euclidean Distance:**\n","\n","Advantages:\n","- Euclidean distance is a straightforward and computationally simple measure of dissimilarity between data points.\n","- It works well when variables are not highly correlated and when the data distribution is not multivariate normal.\n","\n","Limitations:\n","- Euclidean distance does not take into account variable correlations. This can be a disadvantage when dealing with datasets where variables are correlated, as it may provide inaccurate distance measures.\n","\n","**Exploring the Results:**\n","\n","- When using Mahalanobis distance for outlier detection with ideal significance levels such as alpha = 0.05 or 0.1, no outliers were identified. However, to detect the first outlier, a high significance level of alpha = 0.45 had to be used, indicating a high false positive rate.\n","- In contrast, Euclidean distance identified 8 outliers using a threshold based on quartiles and interquartile range (IQR). This approach is less stringent compared to Mahalanobis distance.\n","- To determine which method is better, the validity of the outliers detected by Euclidean distance needs to be verified by domain experts. This validation ensures that the identified outliers are meaningful and relevant to the dataset.\n","- Ultimately, the decision on which method is better depends on the specific characteristics of the dataset and the importance of accounting for variable correlations in outlier detection.\n","\n"],"metadata":{"id":"sU8tkBjOG2Zk"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"w3C8PlFTsA4d"}},{"cell_type":"markdown","source":["10. Evaluate the efficacy of the following (nearest-neighbors-like) classifier. First calculate the mean vector of each species as described at the beginning of this assignment. Next, for each point in the data set (i.e. row in the csv file restricted to the first 4 columns) predict the species of that point to be the species of the mean vector that is the closest to that point. Use Mahalanobis Distance as the proximity measure. Report your results as a 3-by-3 contingency table T, where T[i][j] is the number of instances in which the true species is i and the predicted species is j. Attach a brief interpretation of what this table reveals about how well your classifier has done. Note that we are well aware that this evaluation is being done on the training set; its not meant for drawing conclusions about predictive accuracy, rather only to get an exploratory sense for what the cont9ngency table reveals and what we can conclude from it."],"metadata":{"id":"jeP8cmSPHRxb"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from scipy.spatial.distance import mahalanobis\n","from numpy.linalg import inv\n","\n","# Read the data\n","file_path = '/content/drive/My Drive/Pujitha_kallu_PRDM/Assignment-1/iris.csv'\n","iris_data = pd.read_csv(file_path)\n","\n","# Drop unnecessary columns\n","iris_data = iris_data.drop(columns=[\"id\"])\n","\n","# Compute the mean vector for each species\n","mean_vectors = iris_data.groupby('label').mean()\n","\n","# Initialize the contingency table\n","contingency_table = np.zeros((3, 3), dtype=int)\n","\n","# Calculate the Mahalanobis Distance for each data point\n","for _, row in iris_data.iterrows():\n","    data_point = row[['Sepal length', 'Sepal width', 'Petal length', 'Petal width']]\n","    true_species = row['label']\n","\n","    min_distance = float('inf')\n","    predicted_species = None\n","\n","    # Calculate Mahalanobis Distance to each mean vector\n","    for species, mean_vector in mean_vectors.iterrows():\n","        species_cov_matrix = iris_data[iris_data['label'] == species][['Sepal length', 'Sepal width', 'Petal length', 'Petal width']].cov()\n","        inv_species_cov_matrix = inv(species_cov_matrix)\n","        distance = mahalanobis(data_point, mean_vector, inv_species_cov_matrix)\n","\n","        # Update predicted species if closer\n","        if distance < min_distance:\n","            min_distance = distance\n","            predicted_species = species\n","\n","    # Update contingency table\n","    true_index = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'].index(true_species)\n","    predicted_index = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'].index(predicted_species)\n","    contingency_table[true_index][predicted_index] += 1\n","\n","# Display contingency table\n","print(\"Contingency Table:\")\n","print(contingency_table)\n"],"metadata":{"id":"fM2h-sGrHV_r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714458833264,"user_tz":420,"elapsed":1749,"user":{"displayName":"Pujitha Kallu","userId":"11726106518916817970"}},"outputId":"615c4092-a2b1-4ec6-ef31-22a96fdfeb8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Contingency Table:\n","[[50  0  0]\n"," [ 0 47  3]\n"," [ 0  0 50]]\n"]}]},{"cell_type":"markdown","source":[" Let's analyze the interpretation of each row:\n","\n","1. Row 1: [50, 0, 0]\n","   - True Species: Iris-setosa\n","   - Interpretation: All 50 instances of Iris-setosa in the training data were correctly classified as Iris-setosa by the classifier. There are no misclassifications for this species. This row indicates perfect accuracy for Iris-setosa.\n","\n","2. Row 2: [0, 47, 3]\n","   - True Species: Iris-versicolor\n","   - Interpretation: Out of 50 instances of Iris-versicolor, 47 were correctly classified as Iris-versicolor. However, the classifier misclassified 3 instances of Iris-versicolor as a different species. This suggests a slight difficulty in distinguishing some Iris-versicolor samples from other species.\n","\n","3. Row 3: [0, 0, 50]\n","   - True Species: Iris-virginica\n","   - Interpretation: Similar to Iris-setosa, all 50 instances of Iris-virginica were correctly classified as Iris-virginica. There are no misclassifications for this species. This row indicates perfect accuracy for Iris-virginica.\n","\n","\n","---\n","\n"],"metadata":{"id":"PNpo4rqftynZ"}},{"cell_type":"markdown","source":["References:\n","1. Class notes :PPT slides->proxmity measures\n","2. https://www.sciencedirect.com/topics/computer-science/pearson-correlation\n","3. https://www.datastax.com/guides/what-is-cosine-similarity\n","4. https://www.sciencedirect.com/science/article/abs/pii/S0031320308002057#:~:text=The%20Mahalanobis%20distance%20is%20a,the%20features%20of%20data%20points.\n","5. https://www.sciencedirect.com/topics/mathematics/euclidean-distance"],"metadata":{"id":"J5LDFuvzuBxk"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"v0Gjv7kCwH7C"}}]}